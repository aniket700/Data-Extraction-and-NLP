{
  "cells": [
   {
    "attachments": {},
    "cell_type": "markdown",
    "id": "b9a3d878-38ad-4d97-8154-7656a94cff01",
    "metadata": {},
    "source": [
     "# Data Extraction and NLP\n",
     "\n",
     "### The objective  is to extract textual data articles from the given URL and perform text analysis to compute variables.\n",
     "### Here pandas is used to read excel input given and export the results to `Output  Data Structure.xlsx`. \n",
     "### Request is used to get raw html from the `URL` which are availabe within `input.xlsx`.\n",
     "### BeautifulSoup module of python is used for `web Scraping`."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "fe2aa090-288c-4e3c-8854-aab65fd04104",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "import pandas as pd\n",
     "import requests\n",
     "from bs4 import BeautifulSoup\n",
     "\n",
     "%load_ext lab_black"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "fdec9006-ea88-453c-84d4-183d63567dc3",
    "metadata": {},
    "source": [
     "### NLTK Punkt - This tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words.\n",
     "### NLTK stopwords - Stop words are common words like ‘the’, ‘and’, ‘I’, etc. that are very frequent in text, and so don’t convey insights into the specific topic of a document. We can remove these stop words from the text in a given corpus to clean up the data.\n",
     "### word_tokenize - splits a given sentence into words."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "93b717fd-23ad-4692-89b3-42af1ae474cb",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "import nltk\n",
     "from nltk.corpus import stopwords\n",
     "from nltk.tokenize import word_tokenize\n",
     "\n",
     "nltk.download(\"punkt\", quiet=True)\n",
     "nltk.download(\"stopwords\", quiet=True)"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "331f82d0-e081-4d61-ba7b-349cd78ed31f",
    "metadata": {
     "execution": {
      "iopub.execute_input": "2023-05-02T09:32:02.808461Z",
      "iopub.status.busy": "2023-05-02T09:32:02.808461Z",
      "iopub.status.idle": "2023-05-02T09:32:02.831840Z",
      "shell.execute_reply": "2023-05-02T09:32:02.831840Z",
      "shell.execute_reply.started": "2023-05-02T09:32:02.808461Z"
     },
     "tags": []
    },
    "source": [
     "### sent_tokenize - split a document or paragraph into sentences.\n",
     "### syllables - simple syllable estimator"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "3bba1070-3752-4307-ad8f-0c4a5b6a229a",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "from nltk.tokenize import sent_tokenize\n",
     "import syllables\n",
     "from tqdm import tqdm"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "f2e52e7c-a186-48b8-8487-ea0f4badcd97",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "df = pd.read_excel(\"Input.xlsx\")\n",
     "urls = list(df[\"URL\"])\n",
     "urls[:10], len(urls)"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "bf0ab10b-2f73-4784-8cee-6ef234097675",
    "metadata": {},
    "source": [
     "# Fetching web data (Web Scrapping part)"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "6e7f1842-30cd-4e0a-8cbc-5dd856ba37e8",
    "metadata": {},
    "source": [
     "### web scraping using requests and BeatifulSoap :\n",
     "* finding title of article by h1 tag and obtain content using div tag, class name selectors.\n",
     "* since different articles have different class selector used try exception to get element"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "5368e735-c04a-447c-9cd6-ec8e66780f92",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "def fetch_web_data(url):\n",
     "    class_ = [\"td-post-content tagdiv-type\", \"tdb-block-inner td-fix-index\"]\n",
     "    doc = requests.get(url)\n",
     "    soup = BeautifulSoup(doc.content, \"html.parser\")\n",
     "    title = soup.find(\"h1\")\n",
     "    article = soup.find_all(\"div\", {\"class\": class_[0]})\n",
     "    if article:\n",
     "        res = \" \"\n",
     "        for tag in article:\n",
     "            res += tag.text.strip()\n",
     "    else:\n",
     "        article = soup.find_all(\"div\", {\"class\": class_[1]})\n",
     "        res = \" \"\n",
     "        for tag in article:\n",
     "            res += tag.text.strip()\n",
     "    try:\n",
     "        start = res.index(\"Introduction\")\n",
     "        stop = res.index(\"Blackcoffer Insights\")\n",
     "    except:\n",
     "        start = 0\n",
     "        stop = -1\n",
     "    return title.text + \"\\n\" + res[start:stop]"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "2a6072c8-722e-48af-83f0-41f208dd68fd",
    "metadata": {},
    "source": [
     "# Sentimental Analysis "
    ]
   },
   {
    "cell_type": "markdown",
    "id": "81d2e449-b0db-4ffa-bb69-1a773f94b5b4",
    "metadata": {},
    "source": [
     "## Cleaning using Stop Words Lists "
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "d10e19e6-e67d-4860-92fc-21cfb5033b1e",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "def clean_stop_words(text):\n",
     "    stop_words = set(stopwords.words(\"english\"))\n",
     "    words = text.split()\n",
     "    cleaned_words = [word for word in words if word.lower() not in stop_words]\n",
     "    cleaned_text = \" \".join(cleaned_words)\n",
     "    tokens = word_tokenize(cleaned_text)\n",
     "    return tokens"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "9f55eaf0-5f72-49d6-b8fc-667877c73ae1",
    "metadata": {
     "execution": {
      "iopub.execute_input": "2023-05-02T06:22:06.373852Z",
      "iopub.status.busy": "2023-05-02T06:22:06.373852Z",
      "iopub.status.idle": "2023-05-02T06:22:06.394009Z",
      "shell.execute_reply": "2023-05-02T06:22:06.394009Z",
      "shell.execute_reply.started": "2023-05-02T06:22:06.373852Z"
     },
     "tags": []
    },
    "source": [
     "## Extracting Derived variables"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "99700797-0d20-44b0-8298-d12ca840bec8",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "def get_scores(tokens):\n",
     "    # Creating a dictionary of Positive and Negative words\n",
     "    pos_words = {\n",
     "        \"good\",\n",
     "        \"great\",\n",
     "        \"excellent\",\n",
     "        \"fantastic\",\n",
     "        \"awesome\",\n",
     "        \"wonderful\",\n",
     "        \"amazing\",\n",
     "        \"love\",\n",
     "        \"happy\",\n",
     "        \"joy\",\n",
     "        \"excited\",\n",
     "        \"positive\",\n",
     "    }\n",
     "\n",
     "    neg_words = {\n",
     "        \"bad\",\n",
     "        \"poor\",\n",
     "        \"terrible\",\n",
     "        \"horrible\",\n",
     "        \"awful\",\n",
     "        \"disappoint\",\n",
     "        \"unhappy\",\n",
     "        \"sad\",\n",
     "        \"depressed\",\n",
     "        \"negative\",\n",
     "    }\n",
     "\n",
     "    pos_score = 0\n",
     "    for token in tokens:\n",
     "        if token.lower() in pos_words:\n",
     "            pos_score += 1\n",
     "\n",
     "    neg_score = 0\n",
     "    for token in tokens:\n",
     "        if token.lower() in neg_words:\n",
     "            neg_score -= 1\n",
     "        neg_score *= -1\n",
     "\n",
     "    Polarity_Score = (pos_score - neg_score) / ((pos_score + neg_score) + 0.000001)\n",
     "\n",
     "    Subjectivity_Score = (pos_score + neg_score) / ((len(tokens)) + 0.000001)\n",
     "\n",
     "    return pos_score, neg_score, Polarity_Score, Subjectivity_Score"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "a55ca75f-a4a8-4db6-b767-e684a9806185",
    "metadata": {},
    "source": [
     "# Testing\n",
     "\n",
     "###  Below cell was used for running multiple `Test Cases`"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "afa2275e-90ab-4c89-b9a9-4f7a005d83f4",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "import random\n",
     "\n",
     "\n",
     "def get_article():\n",
     "    n = random.choice(range(len(urls)))\n",
     "    print(urls[n], n)\n",
     "    fetched_article = fetch_web_data(urls[n])\n",
     "    return fetched_article\n",
     "\n",
     "\n",
     "tokens = clean_stop_words(get_article())\n",
     "get_scores(tokens)"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "a960a27f-a84f-4d3f-9047-ef6d69ea3afc",
    "metadata": {},
    "source": [
     "# Analysis of Readability"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "165a2c03-663b-49e5-8c3e-6e96d9f2d5b3",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "def Analysis_of_readability(tokens, fetched_article):\n",
     "    sentences = sent_tokenize(fetched_article)\n",
     "    total_num_of_sentences = len(sentences)\n",
     "    total_num_of_words = len(tokens)\n",
     "\n",
     "    # Estimates the number of syllables in an English-langauge word\n",
     "    num_complex_words = 0\n",
     "    for token in tokens:\n",
     "        if syllables.estimate(token) > 2:\n",
     "            num_complex_words += 1\n",
     "\n",
     "    # Average_Sentence_Length\n",
     "    Average_Sentence_Length = total_num_of_words / total_num_of_sentences\n",
     "    Percentage_of_Complex_words = num_complex_words / total_num_of_words\n",
     "    Fog_Index = 0.4 * (Average_Sentence_Length + Percentage_of_Complex_words)\n",
     "\n",
     "    # Average_Number_of_Words_Per_Sentence\n",
     "    Average_Number_of_Words_Per_Sentence = total_num_of_words / total_num_of_sentences\n",
     "\n",
     "    # SYLLABLE_PER_WORD\n",
     "    total_syllables = sum(syllables.estimate(word) for word in tokens)\n",
     "    SYLLABLE_PER_WORD = total_syllables / total_num_of_words\n",
     "    SYLLABLE_PER_WORD\n",
     "\n",
     "    return (\n",
     "        num_complex_words,\n",
     "        Average_Sentence_Length,\n",
     "        Percentage_of_Complex_words,\n",
     "        Fog_Index,\n",
     "        Average_Number_of_Words_Per_Sentence,\n",
     "        SYLLABLE_PER_WORD,\n",
     "    )"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "e937ae7b-0704-4872-b111-725d752ec6f8",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "def get_personal_pronouns(tokens, num_words):\n",
     "    personal_pronouns = [\n",
     "        \"I\",\n",
     "        \"me\",\n",
     "        \"my\",\n",
     "        \"mine\",\n",
     "        \"you\",\n",
     "        \"your\",\n",
     "        \"yours\",\n",
     "        \"he\",\n",
     "        \"him\",\n",
     "        \"his\",\n",
     "        \"she\",\n",
     "        \"her\",\n",
     "        \"hers\",\n",
     "        \"it\",\n",
     "        \"its\",\n",
     "        \"we\",\n",
     "        \"us\",\n",
     "        \"our\",\n",
     "        \"ours\",\n",
     "        \"they\",\n",
     "        \"them\",\n",
     "        \"their\",\n",
     "        \"theirs\",\n",
     "    ]\n",
     "    # num_personal_pronouns\n",
     "    num_personal_pronouns = sum(\n",
     "        [1 for word in tokens if word.lower() in personal_pronouns]\n",
     "    )\n",
     "\n",
     "    # avg_word_length\n",
     "    total_chars = sum(len(word) for word in tokens)\n",
     "    avg_word_length = total_chars / num_words\n",
     "\n",
     "    return num_personal_pronouns, avg_word_length"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "51e566d6-21d9-468d-a7ad-97faf3fe280e",
    "metadata": {
     "execution": {
      "iopub.execute_input": "2023-05-02T08:09:17.329442Z",
      "iopub.status.busy": "2023-05-02T08:09:17.329442Z",
      "iopub.status.idle": "2023-05-02T08:09:17.361058Z",
      "shell.execute_reply": "2023-05-02T08:09:17.361058Z",
      "shell.execute_reply.started": "2023-05-02T08:09:17.329442Z"
     },
     "tags": []
    },
    "source": [
     "<!-- # 1. All input variables in “Input.xlsx” - columns\n",
     "# 2. POSITIVE SCORE - pos_score\n",
     "# 3. NEGATIVE SCORE - neg_score\n",
     "# 4. POLARITY SCORE - Polarity_Score\n",
     "# 5. SUBJECTIVITY SCORE - Subjectivity_Score\n",
     "# 6. AVG SENTENCE LENGTH - Average_Sentence_Length\n",
     "# 7. PERCENTAGE OF COMPLEX WORDS - Percentage_of_Complex_words\n",
     "# 8. FOG INDEX - Fog_Index\n",
     "# 9. AVG NUMBER OF WORDS PER SENTENCE - Average_Number_of_Words_Per_Sentence\n",
     "# 10. COMPLEX WORD COUNT - num_complex_words\n",
     "# 11. WORD COUNT - total_num_of_words\n",
     "# 12. SYLLABLE PER WORD - SYLLABLE_PER_WORD\n",
     "# 13. PERSONAL PRONOUNS - num_personal_pronouns\n",
     "# 14. AVG WORD LENGTH - avg_word_length -->"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "5aa86cad-88d4-495b-a20a-2a2183d88940",
    "metadata": {},
    "source": [
     "# Obtaining all the variables\n",
     "\n",
     "### By looping through multiple `URLS` we obtain variables by performing various calculations and is stored into list's. The list is inserted into dictionary with the columns as instructed."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "479b6663-56eb-412c-925c-bac979174e9e",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "from nltk.stem import PorterStemmer\n",
     "\n",
     "stemmer = PorterStemmer()"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "fbc898f7-8ea0-4413-9159-b78229fe6ce5",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "fetched_article = input()\n",
     "\n",
     "tokens = clean_stop_words(fetched_article)\n",
     "\n",
     "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
     "\n",
     "print(stemmed_words)\n",
     "\n",
     "total_num_of_words = len(tokens)\n",
     "\n",
     "pos_score, neg_score, Polarity_Score, Subjectivity_Score = get_scores(stemmed_words)\n",
     "\n",
     "(\n",
     "    num_complex_words,\n",
     "    Average_Sentence_Length,\n",
     "    Percentage_of_Complex_words,\n",
     "    Fog_Index,\n",
     "    Average_Number_of_Words_Per_Sentence,\n",
     "    SYLLABLE_PER_WORD,\n",
     ") = Analysis_of_readability(tokens, fetched_article)\n",
     "\n",
     "num_personal_pronouns, avg_word_length = get_personal_pronouns(\n",
     "    tokens, total_num_of_words\n",
     ")\n",
     "\n",
     "print(f\"Positive {pos_score} | Negative {neg_score}\")"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "02c91fad-5b97-4dae-a792-ce2f3b80a635",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "id_r = []\n",
     "url_r = []\n",
     "pos_score_r = []\n",
     "neg_score_r = []\n",
     "Polarity_Score_r = []\n",
     "Polarity_Score_r = []\n",
     "Subjectivity_Score_r = []\n",
     "Average_Sentence_Length_r = []\n",
     "Percentage_of_Complex_words_r = []\n",
     "Fog_Index_r = []\n",
     "Average_Number_of_Words_Per_Sentence_r = []\n",
     "num_complex_words_r = []\n",
     "total_num_of_words_r = []\n",
     "SYLLABLE_PER_WORD_r = []\n",
     "num_personal_pronouns_r = []\n",
     "avg_word_length_r = []\n",
     "\n",
     "\n",
     "# Iterating through URLS\n",
     "\n",
     "for n in tqdm(range(len(urls))):\n",
     "    try:\n",
     "        # fetch_web_data\n",
     "        fetched_article = fetch_web_data(urls[n])\n",
     "    except:\n",
     "        print(f\"Page {urls[n]} Not Found....!\")\n",
     "        continue\n",
     "    index = df.iloc[n]\n",
     "    id_ = index[0]\n",
     "    url_ = index[1]\n",
     "\n",
     "    # clean_stop_words\n",
     "    tokens = clean_stop_words(fetched_article)\n",
     "    total_num_of_words = len(tokens)\n",
     "\n",
     "    pos_score, neg_score, Polarity_Score, Subjectivity_Score = get_scores(tokens)\n",
     "\n",
     "    (\n",
     "        num_complex_words,\n",
     "        Average_Sentence_Length,\n",
     "        Percentage_of_Complex_words,\n",
     "        Fog_Index,\n",
     "        Average_Number_of_Words_Per_Sentence,\n",
     "        SYLLABLE_PER_WORD,\n",
     "    ) = Analysis_of_readability(tokens, fetched_article)\n",
     "\n",
     "    num_personal_pronouns, avg_word_length = get_personal_pronouns(\n",
     "        tokens, total_num_of_words\n",
     "    )\n",
     "\n",
     "    # Appending obtained variables into respective lists\n",
     "    id_r.append(id_)\n",
     "    url_r.append(url_)\n",
     "    pos_score_r.append(pos_score)\n",
     "    neg_score_r.append(neg_score)\n",
     "    Polarity_Score_r.append(Polarity_Score)\n",
     "    Subjectivity_Score_r.append(Subjectivity_Score)\n",
     "    Average_Sentence_Length_r.append(Average_Sentence_Length)\n",
     "    Percentage_of_Complex_words_r.append(Percentage_of_Complex_words)\n",
     "    Fog_Index_r.append(Fog_Index)\n",
     "    Average_Number_of_Words_Per_Sentence_r.append(Average_Number_of_Words_Per_Sentence)\n",
     "    num_complex_words_r.append(num_complex_words)\n",
     "    total_num_of_words_r.append(total_num_of_words)\n",
     "    SYLLABLE_PER_WORD_r.append(SYLLABLE_PER_WORD)\n",
     "    num_personal_pronouns_r.append(num_personal_pronouns)\n",
     "    avg_word_length_r.append(avg_word_length)"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "84733e38-8168-4d18-97fa-7724b0c340c4",
    "metadata": {},
    "source": [
     "# Converting obtained variables into python dictionary"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "415f70a0-9d3d-44ba-a6ed-b21950cf2066",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "output = {\n",
     "    \"URL_ID\": id_r,\n",
     "    \"URL\": url_r,\n",
     "    \"POSITIVE SCORE\": pos_score_r,\n",
     "    \"NEGATIVE SCORE\": neg_score_r,\n",
     "    \"POLARITY SCORE\": Polarity_Score_r,\n",
     "    \"SUBJECTIVITY SCORE\": Subjectivity_Score_r,\n",
     "    \"AVG SENTENCE LENGTH\": Average_Sentence_Length_r,\n",
     "    \"PERCENTAGE OF COMPLEX WORDS\": Percentage_of_Complex_words_r,\n",
     "    \"FOG INDEX\": Fog_Index_r,\n",
     "    \"AVG NUMBER OF WORDS PER SENTENCE\": Average_Number_of_Words_Per_Sentence_r,\n",
     "    \" COMPLEX WORD COUNT\": num_complex_words_r,\n",
     "    \"WORD COUNT \": total_num_of_words_r,\n",
     "    \"SYLLABLE PER WORD \": SYLLABLE_PER_WORD_r,\n",
     "    \"PERSONAL PRONOUNS\": num_personal_pronouns_r,\n",
     "    \"AVG WORD LENGTH\": avg_word_length_r,\n",
     "}\n",
     "\n",
     "output_df = pd.DataFrame(output).set_index(\"URL_ID\")\n",
     "output_df"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "0111265e-aeb0-44f6-b20d-ca2207b1cba5",
    "metadata": {},
    "source": [
     "# Exporting the output_df to excel"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "3f4cf4fb-5b76-4933-88e3-3c6c07f5b33f",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "output_df.to_excel(\"Output Data Structure.xlsx\")"
    ]
   }
  ],
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3 (ipykernel)",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.10.11"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
 }

# Data Extraction and NLP
 
 # Note - `Few web pages where not found during analysis.`
 
 # The coding was done in jupyter because is flexibility compare to scripting.
 
 # Run `requirments.txt` first using `python3 install -r requirements.txt` then run jupyter file
 
 ### The objective of this assignment is to extract textual data articles from the given URL and perform text analysis to compute variables.
 
 ### Here pandas is used to read excel input given and export the results to `Output  Data Structure.xlsx`.
 
 ### Request module is used to get raw html from the `URL` which are availabe within `input.xlsx`.
 
 ### BeautifulSoup module of python is used for `web Scraping`.
 
 ### NLTK Punkt - This tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words.
 
 ### NLTK stopwords - Stop words are common words like ‘the’, ‘and’, ‘I’, etc. that are very frequent in text, and so don’t convey insights into the specific topic of a document. We can remove these stop words from the text in a given corpus to clean up the data.
 
 ### word_tokenize - splits a given sentence into words.
 
 ### sent_tokenize - split a document or paragraph into sentences.
 
 ### syllables - simple syllable estimator
 
 ### web scraping using requests and BeatifulSoap :
 
 - finding title of article by h1 tag and obtain content using div tag, class name selectors.
 - since different articles have different class selector used try exception to get element
 
 # Obtaining all the variables
 
 ### By looping through multiple `URLS` we obtain variables by performing various calculations and is stored into list's. The list is inserted into dictionary with the columns as instructed.

pandas
 bs4
 nltk
