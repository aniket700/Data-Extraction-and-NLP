{
  "cells": [
   {
    "attachments": {},
    "cell_type": "markdown",
    "id": "b9a3d878-38ad-4d97-8154-7656a94cff01",
    "metadata": {},
    "source": [
     "# Data Extraction and NLP\n",
     "\n",
     "### The objective  is to extract textual data articles from the given URL and perform text analysis to compute variables.\n",
     "### Here pandas is used to read excel input given and export the results to `Output  Data Structure.xlsx`. \n",
     "### Request is used to get raw html from the `URL` which are availabe within `input.xlsx`.\n",
     "### BeautifulSoup module of python is used for `web Scraping`."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "execution_count": 1,
    "id": "fe2aa090-288c-4e3c-8854-aab65fd04104",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "import os\n",
     "import random\n",
     "import re\n",
     "import warnings\n",
     "\n",
     "import pandas as pd\n",
     "import requests\n",
     "import syllables\n",
     "from bs4 import BeautifulSoup\n",
     "from nltk.corpus import stopwords as nltk_sw\n",
     "\n",
     "%load_ext lab_black"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "fdec9006-ea88-453c-84d4-183d63567dc3",
    "metadata": {},
    "source": [
     "### NLTK Punkt - This tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words.\n",
     "### NLTK stopwords - Stop words are common words like ‘the’, ‘and’, ‘I’, etc. that are very frequent in text, and so don’t convey insights into the specific topic of a document. We can remove these stop words from the text in a given corpus to clean up the data.\n",
     "### word_tokenize - splits a given sentence into words."
     "warnings.filterwarnings(\"ignore\")"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "93b717fd-23ad-4692-89b3-42af1ae474cb",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "import nltk\n",
     "from nltk.corpus import stopwords\n",
     "from nltk.tokenize import word_tokenize\n",
     "\n",
     "nltk.download(\"punkt\", quiet=True)\n",
     "nltk.download(\"stopwords\", quiet=True)"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "331f82d0-e081-4d61-ba7b-349cd78ed31f",
    "metadata": {
     "execution": {
      "iopub.execute_input": "2023-05-02T09:32:02.808461Z",
      "iopub.status.busy": "2023-05-02T09:32:02.808461Z",
      "iopub.status.idle": "2023-05-02T09:32:02.831840Z",
      "shell.execute_reply": "2023-05-02T09:32:02.831840Z",
      "shell.execute_reply.started": "2023-05-02T09:32:02.808461Z"
     },
     "tags": []
    },
    "source": [
     "### sent_tokenize - split a document or paragraph into sentences.\n",
     "### syllables - simple syllable estimator"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "3bba1070-3752-4307-ad8f-0c4a5b6a229a",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "from nltk.tokenize import sent_tokenize\n",
     "import syllables\n",
     "from tqdm import tqdm"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "execution_count": 2,
    "id": "f2e52e7c-a186-48b8-8487-ea0f4badcd97",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "outputs": [
     {
      "data": {
       "text/plain": [
        "(['https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-3-2/',\n",
        "  'https://insights.blackcoffer.com/rise-of-e-health-and-its-impact-on-humans-by-the-year-2030/',\n",
        "  'https://insights.blackcoffer.com/rise-of-e-health-and-its-imapct-on-humans-by-the-year-2030-2/',\n",
        "  'https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-2/',\n",
        "  'https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-2-2/',\n",
        "  'https://insights.blackcoffer.com/rise-of-chatbots-and-its-impact-on-customer-support-by-the-year-2040/',\n",
        "  'https://insights.blackcoffer.com/rise-of-e-health-and-its-imapct-on-humans-by-the-year-2030/',\n",
        "  'https://insights.blackcoffer.com/how-does-marketing-influence-businesses-and-consumers/',\n",
        "  'https://insights.blackcoffer.com/how-advertisement-increase-your-market-value/',\n",
        "  'https://insights.blackcoffer.com/negative-effects-of-marketing-on-society/'],\n",
        " 114)"
       ]
      },
      "execution_count": 2,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "df = pd.read_excel(\"Input.xlsx\")\n",
     "urls = list(df[\"URL\"])\n",
 @@ -105,30 +60,31 @@
   },
   {
    "cell_type": "markdown",
    "id": "bf0ab10b-2f73-4784-8cee-6ef234097675",
    "id": "096584fc-3007-4b37-b77e-1f9f9abe6a72",
    "metadata": {},
    "source": [
     "# Fetching web data (Web Scrapping part)"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "6e7f1842-30cd-4e0a-8cbc-5dd856ba37e8",
    "metadata": {},
    "source": [
     "### web scraping using requests and BeatifulSoap :\n",
     "* finding title of article by h1 tag and obtain content using div tag, class name selectors.\n",
     "* since different articles have different class selector used try exception to get element"
     "# fetch_web_data"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "execution_count": 3,
    "id": "5368e735-c04a-447c-9cd6-ec8e66780f92",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "outputs": [
     {
      "data": {
       "text/plain": [
        "'Oil prices by the year 2040, and how it will impact the world economy.\\n We are in an interconnected world. Any change in one part of the world will always lead to some #changes in other parts of the world as well, maybe a bit later but surely there will be some change and that is what we are seeing in today’s world. Electric vehicles are the change that we are seeing in today’s world. With so many advancements in technology, economies are getting bigger. China might surpass the US and become a #'"
       ]
      },
      "execution_count": 3,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "def fetch_web_data(url):\n",
     "    class_ = [\"td-post-content tagdiv-type\", \"tdb-block-inner td-fix-index\"]\n",
 @@ -151,188 +107,238 @@
     "    except:\n",
     "        start = 0\n",
     "        stop = -1\n",
     "    return title.text + \"\\n\" + res[start:stop]"
     "    return title.text + \"\\n\" + res[start:stop]\n",
     "\n",
     "\n",
     "fetch_web_data(random.choice(urls))[:500]"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "2a6072c8-722e-48af-83f0-41f208dd68fd",
    "id": "b449e93b-b3b8-4f3e-865c-b09cb4cfaf79",
    "metadata": {},
    "source": [
     "# Sentimental Analysis "
     "# Stop Words"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "81d2e449-b0db-4ffa-bb69-1a773f94b5b4",
    "cell_type": "code",
    "execution_count": 13,
    "id": "906f60f3-9387-4b4e-bb23-7f3db58bc35d",
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "['ERNST',\n",
        " 'YOUNG',\n",
        " 'DELOITTE',\n",
        " 'TOUCHE',\n",
        " 'KPMG',\n",
        " 'PRICEWATERHOUSECOOPERS',\n",
        " 'PRICEWATERHOUSE',\n",
        " 'COOPERS',\n",
        " 'AFGHANI',\n",
        " 'ARIARY']"
       ]
      },
      "execution_count": 13,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "## Cleaning using Stop Words Lists "
     "def get_stop_words():\n",
     "    StopWords_notNames = []\n",
     "    for file in os.listdir(\"StopWords/\"):\n",
     "        if file != \"StopWords_Names.txt\":\n",
     "            corpus = open(f\"StopWords/{file}\", \"r\").read().strip(\" \").split(\"\\n\")\n",
     "            res = []\n",
     "            for txt in corpus:\n",
     "                if \"|\" in txt:\n",
     "                    res.extend(txt.replace(\" | \", \",\").replace(\" \", \"\").split(\",\"))\n",
     "            if res != []:\n",
     "                StopWords_notNames.extend(res)\n",
     "\n",
     "    StopWords_Names = []\n",
     "    for file in os.listdir(\"StopWords/\"):\n",
     "        if file == \"StopWords_Names.txt\":\n",
     "            corpus = open(f\"StopWords/{file}\", \"r\").read().strip(\" \").split(\"\\n\")\n",
     "            for txt in corpus:\n",
     "                if \"|\" in txt:\n",
     "                    res = txt.replace(\" | \", \",\").replace(\" \", \"\").split(\",\")\n",
     "                    if res != None:\n",
     "                        StopWords_Names.append(res[0])\n",
     "\n",
     "    stop_words = []\n",
     "    for file in os.listdir(\"StopWords/\"):\n",
     "        corpus = open(f\"StopWords/{file}\", \"r\").read().strip().split(\"\\n\")\n",
     "        res = []\n",
     "        for txt in corpus:\n",
     "            if \"|\" in txt:\n",
     "                txt = txt.replace(txt, txt.split(\"|\")[0])\n",
     "                res.append(txt.strip())\n",
     "        if res != []:\n",
     "            stop_words.extend(res)\n",
     "        stop_words.extend([txt for txt in corpus if \"|\" not in txt])\n",
     "\n",
     "    stop_words.extend(StopWords_notNames)\n",
     "    stop_words.extend(StopWords_Names)\n",
     "    return stop_words\n",
     "\n",
     "\n",
     "get_stop_words()[:10]"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "execution_count": 15,
    "id": "d10e19e6-e67d-4860-92fc-21cfb5033b1e",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "outputs": [
     {
      "data": {
       "text/plain": [
        "'machine replace human future work Introduction disruptive technology taking us leave it disruptive technology creates jobs depleted jobs. notice jobs disappearing jobs jobs transform humans robots machines technology creating machines replace them. Technology creates data analysis tools manipulate create custom scenarios artificial intelligence AI Big Data Machine Learning ML algorithms predict drive consumer behavior. Data Analytics tools Google Analytics today free and correctly organizations '"
       ]
      },
      "execution_count": 15,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "def clean_stop_words(text):\n",
     "    stop_words = set(stopwords.words(\"english\"))\n",
     "def clean_stop_words(text, personalwords=True):\n",
     "    stop_words=get_stop_words()\n",
     "    if personalwords == True:\n",
     "        stop_words.extend(nltk_sw.words(\"english\"))\n",
     "    words = text.split()\n",
     "    cleaned_words = [word for word in words if word.lower() not in stop_words]\n",
     "    cleaned_text = \" \".join(cleaned_words)\n",
     "    tokens = word_tokenize(cleaned_text)\n",
     "    return tokens"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "9f55eaf0-5f72-49d6-b8fc-667877c73ae1",
    "metadata": {
     "execution": {
      "iopub.execute_input": "2023-05-02T06:22:06.373852Z",
      "iopub.status.busy": "2023-05-02T06:22:06.373852Z",
      "iopub.status.idle": "2023-05-02T06:22:06.394009Z",
      "shell.execute_reply": "2023-05-02T06:22:06.394009Z",
      "shell.execute_reply.started": "2023-05-02T06:22:06.373852Z"
     },
     "tags": []
    },
    "source": [
     "## Extracting Derived variables"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "99700797-0d20-44b0-8298-d12ca840bec8",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "def get_scores(tokens):\n",
     "    # Creating a dictionary of Positive and Negative words\n",
     "    pos_words = {\n",
     "        \"good\",\n",
     "        \"great\",\n",
     "        \"excellent\",\n",
     "        \"fantastic\",\n",
     "        \"awesome\",\n",
     "        \"wonderful\",\n",
     "        \"amazing\",\n",
     "        \"love\",\n",
     "        \"happy\",\n",
     "        \"joy\",\n",
     "        \"excited\",\n",
     "        \"positive\",\n",
     "    }\n",
     "\n",
     "    neg_words = {\n",
     "        \"bad\",\n",
     "        \"poor\",\n",
     "        \"terrible\",\n",
     "        \"horrible\",\n",
     "        \"awful\",\n",
     "        \"disappoint\",\n",
     "        \"unhappy\",\n",
     "        \"sad\",\n",
     "        \"depressed\",\n",
     "        \"negative\",\n",
     "    }\n",
     "\n",
     "    pos_score = 0\n",
     "    for token in tokens:\n",
     "        if token.lower() in pos_words:\n",
     "            pos_score += 1\n",
     "\n",
     "    neg_score = 0\n",
     "    for token in tokens:\n",
     "        if token.lower() in neg_words:\n",
     "            neg_score -= 1\n",
     "        neg_score *= -1\n",
     "\n",
     "    Polarity_Score = (pos_score - neg_score) / ((pos_score + neg_score) + 0.000001)\n",
     "\n",
     "    Subjectivity_Score = (pos_score + neg_score) / ((len(tokens)) + 0.000001)\n",
     "\n",
     "    return pos_score, neg_score, Polarity_Score, Subjectivity_Score"
     "    cleaned_text = \" \".join(re.findall(\"[a-zA-Z.]+\", cleaned_text))\n",
     "    return cleaned_text\n",
     "\n",
     "\n",
     "clean_stop_words(fetch_web_data(random.choice(urls)))[:500]"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "a55ca75f-a4a8-4db6-b767-e684a9806185",
    "id": "8778b92e-42bb-4fe2-8d93-f1c093faff24",
    "metadata": {},
    "source": [
     "# Testing\n",
     "\n",
     "###  Below cell was used for running multiple `Test Cases`"
     "# Scores"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "afa2275e-90ab-4c89-b9a9-4f7a005d83f4",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "execution_count": 16,
    "id": "7fde7e1c-d74b-46ba-acb9-f684f08eaf40",
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
        "(25, 0, 0.9615384615384616, 0.6352941176470588)"
       ]
      },
      "execution_count": 16,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "import random\n",
     "\n",
     "\n",
     "def get_article():\n",
     "    n = random.choice(range(len(urls)))\n",
     "    print(urls[n], n)\n",
     "    fetched_article = fetch_web_data(urls[n])\n",
     "    return fetched_article\n",
     "\n",
     "\n",
     "tokens = clean_stop_words(get_article())\n",
     "get_scores(tokens)"
     "def get_scores(text):\n",
     "    def get_subjectivity_score(text):\n",
     "        num_words = len(text.split())\n",
     "        unique_words = len(set(text.split()))\n",
     "        subjectivity_score = unique_words / num_words\n",
     "        return subjectivity_score\n",
     "\n",
     "    def get_polarity_score(text):\n",
     "        positive_words = (\n",
     "            open(\"MasterDictionary/positive-words.txt\", \"r\").read().split(\"\\n\")\n",
     "        )\n",
     "        negative_words = (\n",
     "            open(\"MasterDictionary/negative-words.txt\", \"r\").read().split(\"\\n\")\n",
     "        )\n",
     "\n",
     "        positive_count = 0\n",
     "        negative_count = 0\n",
     "\n",
     "        for word in text.split():\n",
     "            if word.lower() in positive_words:\n",
     "                positive_count += 1\n",
     "            elif word.lower() in negative_words:\n",
     "                negative_count += 1\n",
     "\n",
     "        polarity_score = (positive_count - negative_count) / (\n",
     "            positive_count + negative_count + 1\n",
     "        )\n",
     "        return polarity_score, positive_count, negative_count\n",
     "\n",
     "    subjectivity_score = get_subjectivity_score(text)\n",
     "    polarity_score, positive_count, negative_count = get_polarity_score(text)\n",
     "\n",
     "    return positive_count, negative_count, polarity_score, subjectivity_score\n",
     "\n",
     "\n",
     "get_scores(clean_stop_words(fetch_web_data(random.choice(urls))))"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "a960a27f-a84f-4d3f-9047-ef6d69ea3afc",
    "id": "a0c6d820-5482-4b94-93b1-2813a2cd759d",
    "metadata": {},
    "source": [
     "# Analysis of Readability"
     "# Analysis_of_readability"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "execution_count": 17,
    "id": "165a2c03-663b-49e5-8c3e-6e96d9f2d5b3",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "outputs": [
     {
      "data": {
       "text/plain": [
        "(77,\n",
        " 13.73076923076923,\n",
        " 0.0718954248366013,\n",
        " 5.521065862242333,\n",
        " 13.73076923076923,\n",
        " 2.4379084967320264)"
       ]
      },
      "execution_count": 17,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "def Analysis_of_readability(tokens, fetched_article):\n",
     "    sentences = sent_tokenize(fetched_article)\n",
     "def Analysis_of_readability(fetched_article):\n",
     "    sentences = fetched_article.replace(\" \", \"\").split(\".\")\n",
     "    tokens = fetched_article.split(\" \")\n",
     "    total_num_of_sentences = len(sentences)\n",
     "    total_num_of_words = len(tokens)\n",
     "\n",
     "    # Estimates the number of syllables in an English-langauge word\n",
     "    num_complex_words = 0\n",
     "    for token in tokens:\n",
     "    for token in sentences:\n",
     "        if syllables.estimate(token) > 2:\n",
     "            num_complex_words += 1\n",
     "\n",
     "    # Average_Sentence_Length\n",
     "    Average_Sentence_Length = total_num_of_words / total_num_of_sentences\n",
     "    Percentage_of_Complex_words = num_complex_words / total_num_of_words\n",
     "    Fog_Index = 0.4 * (Average_Sentence_Length + Percentage_of_Complex_words)\n",
     "\n",
     "    # Average_Number_of_Words_Per_Sentence\n",
     "    Average_Number_of_Words_Per_Sentence = total_num_of_words / total_num_of_sentences\n",
     "\n",
     "    # SYLLABLE_PER_WORD\n",
     "    total_syllables = sum(syllables.estimate(word) for word in tokens)\n",
     "    total_syllables = sum(syllables.estimate(word) for word in sentences)\n",
     "    SYLLABLE_PER_WORD = total_syllables / total_num_of_words\n",
     "    SYLLABLE_PER_WORD\n",
     "\n",
 @@ -343,19 +349,41 @@
     "        Fog_Index,\n",
     "        Average_Number_of_Words_Per_Sentence,\n",
     "        SYLLABLE_PER_WORD,\n",
     "    )"
     "    )\n",
     "\n",
     "\n",
     "Analysis_of_readability(clean_stop_words(fetch_web_data(random.choice(urls))))"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "1bb50f10-e36e-4725-9fd9-b970f19bf719",
    "metadata": {},
    "source": [
     "# get_personal_pronouns"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "execution_count": 19,
    "id": "e937ae7b-0704-4872-b111-725d752ec6f8",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "outputs": [
     {
      "data": {
       "text/plain": [
        "(1, 6.93010752688172)"
       ]
      },
      "execution_count": 19,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "def get_personal_pronouns(tokens, num_words):\n",
     "def get_personal_pronouns(tokens):\n",
     "    personal_pronouns = [\n",
     "        \"I\",\n",
     "        \"me\",\n",
 @@ -381,16 +409,20 @@
     "        \"their\",\n",
     "        \"theirs\",\n",
     "    ]\n",
     "    # num_personal_pronouns\n",
     "    num_personal_pronouns = sum(\n",
     "        [1 for word in tokens if word.lower() in personal_pronouns]\n",
     "    )\n",
     "\n",
     "    # avg_word_length\n",
     "    total_chars = sum(len(word) for word in tokens)\n",
     "    avg_word_length = total_chars / num_words\n",
     "    avg_word_length = total_chars / len(tokens)\n",
     "\n",
     "    return num_personal_pronouns, avg_word_length"
     "    return num_personal_pronouns, avg_word_length\n",
     "\n",
     "\n",
     "corpus = clean_stop_words(fetch_web_data(random.choice(urls)),personalwords=False)\n",
     "\n",
     "res = re.findall(\"[A-Za-z]+\", corpus)\n",
     "get_personal_pronouns(res)"
    ]
   },
   {
 @@ -425,73 +457,327 @@
   },
   {
    "cell_type": "markdown",
    "id": "5aa86cad-88d4-495b-a20a-2a2183d88940",
    "id": "3ac9a54e-fe67-401e-a011-35bb60dc80db",
    "metadata": {},
    "source": [
     "# Obtaining all the variables\n",
     "\n",
     "### By looping through multiple `URLS` we obtain variables by performing various calculations and is stored into list's. The list is inserted into dictionary with the columns as instructed."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "479b6663-56eb-412c-925c-bac979174e9e",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "from nltk.stem import PorterStemmer\n",
     "\n",
     "stemmer = PorterStemmer()"
     "# Main"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "fbc898f7-8ea0-4413-9159-b78229fe6ce5",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "fetched_article = input()\n",
     "\n",
     "tokens = clean_stop_words(fetched_article)\n",
     "\n",
     "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
     "\n",
     "print(stemmed_words)\n",
     "\n",
     "total_num_of_words = len(tokens)\n",
     "\n",
     "pos_score, neg_score, Polarity_Score, Subjectivity_Score = get_scores(stemmed_words)\n",
     "\n",
     "(\n",
     "    num_complex_words,\n",
     "    Average_Sentence_Length,\n",
     "    Percentage_of_Complex_words,\n",
     "    Fog_Index,\n",
     "    Average_Number_of_Words_Per_Sentence,\n",
     "    SYLLABLE_PER_WORD,\n",
     ") = Analysis_of_readability(tokens, fetched_article)\n",
     "\n",
     "num_personal_pronouns, avg_word_length = get_personal_pronouns(\n",
     "    tokens, total_num_of_words\n",
     ")\n",
     "\n",
     "print(f\"Positive {pos_score} | Negative {neg_score}\")"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "execution_count": 20,
    "id": "02c91fad-5b97-4dae-a792-ce2f3b80a635",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "Page https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/ Not Found....!\n",
       "Page https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/ Not Found....!\n"
      ]
     },
     {
      "data": {
       "text/html": [
        "<div>\n",
        "<style scoped>\n",
        "    .dataframe tbody tr th:only-of-type {\n",
        "        vertical-align: middle;\n",
        "    }\n",
        "\n",
        "    .dataframe tbody tr th {\n",
        "        vertical-align: top;\n",
        "    }\n",
        "\n",
        "    .dataframe thead th {\n",
        "        text-align: right;\n",
        "    }\n",
        "</style>\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>POSITIVE SCORE</th>\n",
        "      <th>NEGATIVE SCORE</th>\n",
        "      <th>POLARITY SCORE</th>\n",
        "      <th>SUBJECTIVITY SCORE</th>\n",
        "      <th>AVG SENTENCE LENGTH</th>\n",
        "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
        "      <th>FOG INDEX</th>\n",
        "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
        "      <th>COMPLEX WORD COUNT</th>\n",
        "      <th>WORD COUNT</th>\n",
        "      <th>SYLLABLE PER WORD</th>\n",
        "      <th>PERSONAL PRONOUNS</th>\n",
        "      <th>AVG WORD LENGTH</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>URL_ID</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>123.0</th>\n",
        "      <td>75</td>\n",
        "      <td>21</td>\n",
        "      <td>0.556701</td>\n",
        "      <td>0.398870</td>\n",
        "      <td>18.896552</td>\n",
        "      <td>0.052311</td>\n",
        "      <td>7.579545</td>\n",
        "      <td>18.896552</td>\n",
        "      <td>86</td>\n",
        "      <td>7746</td>\n",
        "      <td>2.003041</td>\n",
        "      <td>2</td>\n",
        "      <td>7.639233</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>321.0</th>\n",
        "      <td>38</td>\n",
        "      <td>12</td>\n",
        "      <td>0.509804</td>\n",
        "      <td>0.534426</td>\n",
        "      <td>24.760000</td>\n",
        "      <td>0.040388</td>\n",
        "      <td>9.920155</td>\n",
        "      <td>24.760000</td>\n",
        "      <td>25</td>\n",
        "      <td>2675</td>\n",
        "      <td>1.854604</td>\n",
        "      <td>1</td>\n",
        "      <td>7.695082</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2345.0</th>\n",
        "      <td>22</td>\n",
        "      <td>20</td>\n",
        "      <td>0.046512</td>\n",
        "      <td>0.649266</td>\n",
        "      <td>15.554054</td>\n",
        "      <td>0.063423</td>\n",
        "      <td>6.246991</td>\n",
        "      <td>15.554054</td>\n",
        "      <td>73</td>\n",
        "      <td>5022</td>\n",
        "      <td>1.771503</td>\n",
        "      <td>0</td>\n",
        "      <td>7.168595</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4321.0</th>\n",
        "      <td>34</td>\n",
        "      <td>26</td>\n",
        "      <td>0.131148</td>\n",
        "      <td>0.670504</td>\n",
        "      <td>20.576271</td>\n",
        "      <td>0.048600</td>\n",
        "      <td>8.249948</td>\n",
        "      <td>20.576271</td>\n",
        "      <td>59</td>\n",
        "      <td>5702</td>\n",
        "      <td>1.894563</td>\n",
        "      <td>1</td>\n",
        "      <td>7.153179</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>432.0</th>\n",
        "      <td>34</td>\n",
        "      <td>26</td>\n",
        "      <td>0.131148</td>\n",
        "      <td>0.670504</td>\n",
        "      <td>20.576271</td>\n",
        "      <td>0.048600</td>\n",
        "      <td>8.249948</td>\n",
        "      <td>20.576271</td>\n",
        "      <td>59</td>\n",
        "      <td>5702</td>\n",
        "      <td>1.894563</td>\n",
        "      <td>1</td>\n",
        "      <td>7.153179</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>...</th>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50921.0</th>\n",
        "      <td>5</td>\n",
        "      <td>27</td>\n",
        "      <td>-0.666667</td>\n",
        "      <td>0.729592</td>\n",
        "      <td>19.363636</td>\n",
        "      <td>0.050078</td>\n",
        "      <td>7.765486</td>\n",
        "      <td>19.363636</td>\n",
        "      <td>32</td>\n",
        "      <td>3024</td>\n",
        "      <td>1.902973</td>\n",
        "      <td>0</td>\n",
        "      <td>6.738342</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>51382.8</th>\n",
        "      <td>24</td>\n",
        "      <td>59</td>\n",
        "      <td>-0.416667</td>\n",
        "      <td>0.532274</td>\n",
        "      <td>30.450980</td>\n",
        "      <td>0.032840</td>\n",
        "      <td>12.193528</td>\n",
        "      <td>30.450980</td>\n",
        "      <td>51</td>\n",
        "      <td>7526</td>\n",
        "      <td>1.912428</td>\n",
        "      <td>0</td>\n",
        "      <td>6.444223</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>51844.6</th>\n",
        "      <td>81</td>\n",
        "      <td>28</td>\n",
        "      <td>0.481818</td>\n",
        "      <td>0.698492</td>\n",
        "      <td>23.972222</td>\n",
        "      <td>0.041136</td>\n",
        "      <td>9.605343</td>\n",
        "      <td>23.972222</td>\n",
        "      <td>71</td>\n",
        "      <td>7762</td>\n",
        "      <td>1.792584</td>\n",
        "      <td>10</td>\n",
        "      <td>6.723896</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>52306.4</th>\n",
        "      <td>32</td>\n",
        "      <td>22</td>\n",
        "      <td>0.181818</td>\n",
        "      <td>0.620419</td>\n",
        "      <td>19.921875</td>\n",
        "      <td>0.047059</td>\n",
        "      <td>7.987574</td>\n",
        "      <td>19.921875</td>\n",
        "      <td>60</td>\n",
        "      <td>5687</td>\n",
        "      <td>1.879216</td>\n",
        "      <td>7</td>\n",
        "      <td>6.345953</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>52768.2</th>\n",
        "      <td>51</td>\n",
        "      <td>30</td>\n",
        "      <td>0.256098</td>\n",
        "      <td>0.766436</td>\n",
        "      <td>22.238095</td>\n",
        "      <td>0.044968</td>\n",
        "      <td>8.913225</td>\n",
        "      <td>22.238095</td>\n",
        "      <td>42</td>\n",
        "      <td>5016</td>\n",
        "      <td>2.175589</td>\n",
        "      <td>2</td>\n",
        "      <td>7.595855</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>112 rows × 13 columns</p>\n",
        "</div>"
       ],
       "text/plain": [
        "         POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
        "URL_ID                                                                        \n",
        "123.0                75              21        0.556701            0.398870   \n",
        "321.0                38              12        0.509804            0.534426   \n",
        "2345.0               22              20        0.046512            0.649266   \n",
        "4321.0               34              26        0.131148            0.670504   \n",
        "432.0                34              26        0.131148            0.670504   \n",
        "...                 ...             ...             ...                 ...   \n",
        "50921.0               5              27       -0.666667            0.729592   \n",
        "51382.8              24              59       -0.416667            0.532274   \n",
        "51844.6              81              28        0.481818            0.698492   \n",
        "52306.4              32              22        0.181818            0.620419   \n",
        "52768.2              51              30        0.256098            0.766436   \n",
        "\n",
        "         AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
        "URL_ID                                                                 \n",
        "123.0              18.896552                     0.052311   7.579545   \n",
        "321.0              24.760000                     0.040388   9.920155   \n",
        "2345.0             15.554054                     0.063423   6.246991   \n",
        "4321.0             20.576271                     0.048600   8.249948   \n",
        "432.0              20.576271                     0.048600   8.249948   \n",
        "...                      ...                          ...        ...   \n",
        "50921.0            19.363636                     0.050078   7.765486   \n",
        "51382.8            30.450980                     0.032840  12.193528   \n",
        "51844.6            23.972222                     0.041136   9.605343   \n",
        "52306.4            19.921875                     0.047059   7.987574   \n",
        "52768.2            22.238095                     0.044968   8.913225   \n",
        "\n",
        "         AVG NUMBER OF WORDS PER SENTENCE   COMPLEX WORD COUNT  WORD COUNT   \\\n",
        "URL_ID                                                                        \n",
        "123.0                           18.896552                   86         7746   \n",
        "321.0                           24.760000                   25         2675   \n",
        "2345.0                          15.554054                   73         5022   \n",
        "4321.0                          20.576271                   59         5702   \n",
        "432.0                           20.576271                   59         5702   \n",
        "...                                   ...                  ...          ...   \n",
        "50921.0                         19.363636                   32         3024   \n",
        "51382.8                         30.450980                   51         7526   \n",
        "51844.6                         23.972222                   71         7762   \n",
        "52306.4                         19.921875                   60         5687   \n",
        "52768.2                         22.238095                   42         5016   \n",
        "\n",
        "         SYLLABLE PER WORD   PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
        "URL_ID                                                           \n",
        "123.0              2.003041                  2         7.639233  \n",
        "321.0              1.854604                  1         7.695082  \n",
        "2345.0             1.771503                  0         7.168595  \n",
        "4321.0             1.894563                  1         7.153179  \n",
        "432.0              1.894563                  1         7.153179  \n",
        "...                     ...                ...              ...  \n",
        "50921.0            1.902973                  0         6.738342  \n",
        "51382.8            1.912428                  0         6.444223  \n",
        "51844.6            1.792584                 10         6.723896  \n",
        "52306.4            1.879216                  7         6.345953  \n",
        "52768.2            2.175589                  2         7.595855  \n",
        "\n",
        "[112 rows x 13 columns]"
       ]
      },
      "execution_count": 20,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
     "id_r = []\n",
     "url_r = []\n",
 @@ -512,8 +798,7 @@
     "\n",
     "\n",
     "# Iterating through URLS\n",
     "\n",
     "for n in tqdm(range(len(urls))):\n",
     "for n in range(len(urls)):\n",
     "    try:\n",
     "        # fetch_web_data\n",
     "        fetched_article = fetch_web_data(urls[n])\n",
 @@ -537,11 +822,11 @@
     "        Fog_Index,\n",
     "        Average_Number_of_Words_Per_Sentence,\n",
     "        SYLLABLE_PER_WORD,\n",
     "    ) = Analysis_of_readability(tokens, fetched_article)\n",
     "    ) = Analysis_of_readability(fetched_article)\n",
     "\n",
     "    num_personal_pronouns, avg_word_length = get_personal_pronouns(\n",
     "        tokens, total_num_of_words\n",
     "    )\n",
     "    tmp=clean_stop_words(fetched_article,personalwords=False)\n",
     "    res = re.findall(\"[A-Za-z]+\", tmp)\n",
     "    num_personal_pronouns, avg_word_length = get_personal_pronouns(res)\n",
     "\n",
     "    # Appending obtained variables into respective lists\n",
     "    id_r.append(id_)\n",
 @@ -558,29 +843,11 @@
     "    total_num_of_words_r.append(total_num_of_words)\n",
     "    SYLLABLE_PER_WORD_r.append(SYLLABLE_PER_WORD)\n",
     "    num_personal_pronouns_r.append(num_personal_pronouns)\n",
     "    avg_word_length_r.append(avg_word_length)"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "84733e38-8168-4d18-97fa-7724b0c340c4",
    "metadata": {},
    "source": [
     "# Converting obtained variables into python dictionary"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "415f70a0-9d3d-44ba-a6ed-b21950cf2066",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "    avg_word_length_r.append(avg_word_length)\n",
     "\n",
     "\n",
     "output = {\n",
     "    \"URL_ID\": id_r,\n",
     "    \"URL\": url_r,\n",
     "    \"POSITIVE SCORE\": pos_score_r,\n",
     "    \"NEGATIVE SCORE\": neg_score_r,\n",
     "    \"POLARITY SCORE\": Polarity_Score_r,\n",
 @@ -610,14 +877,14 @@
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "execution_count": 21,
    "id": "3f4cf4fb-5b76-4933-88e3-3c6c07f5b33f",
    "metadata": {
     "tags": []
    },
    "outputs": [],
    "source": [
     "output_df.to_excel(\"Output Data Structure.xlsx\")"
     "output_df.to_excel(\"Output Data.xlsx\")"
    ]
   }
  ],
 @@ -637,7 +904,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.10.11"
    "version": "3.11.5"
   }
  },
  "nbformat": 4,
